{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WW3_Downloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzdVBHiSBCzm5z1B2LLM+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlosenciso/WW3ModelPrePost/blob/master/NOTEBOOK/WW3_Downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB9pnpNoJT8U"
      },
      "source": [
        "----\n",
        "<div style=\"width: 100%; overflow: hidden;\">\n",
        "    <div style=\"width: 400px; float: left;\"> </div>\n",
        "    <div style=\"float: left; margin-left: 10px;\"> <h1><strong>WaveWatch Model III Downloader</strong></h1>\n",
        "<h1><strong>Pre/Post Processing</strong></h1>\n",
        "        <p><strong>Created by:</strong> Carlos Enciso Ojeda</p><br>\n",
        "        <strong>Atmopsheric and Climate Research</strong></br>\n",
        "        <strong>Last modified time: 2021-03-30 T18:00:14-05:00<strong><br>\n",
        "        <a href=\"https://github.com/carlosenciso/\">www.atmcenciso.com</a><br/>\n",
        "        <strong>Email:<strong> carlos.enciso.o@gmail.com | cenciso@senamhi.gob.pe</p></div>\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVPabFFQQVOb",
        "cellView": "form"
      },
      "source": [
        "#@title Install Pkgs Unix!.\n",
        "#---- Install Dependencies ----#\n",
        "!pip install cfgrib\n",
        "!apt install libeccodes-tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmcnzaxXKnqF"
      },
      "source": [
        "## **1. Importing modules**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVB3Md82K6Lk",
        "cellView": "form"
      },
      "source": [
        "#@title Start modules!.\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import drive\n",
        "from dask.diagnostics import ProgressBar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrosyKUFCvKM"
      },
      "source": [
        "drive.mount(\"/content/drive\",force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVI653iBsWBf"
      },
      "source": [
        "## **2. Downloader WW3 model**\n",
        "Dataset URL's was getting from https://polar.ncep.noaa.gov/waves/hindcasts/ website for hindcast. After hindcast dataset was getting from https://pae-paha.pacioos.hawaii.edu/thredds/dodsC/ww3_global/WaveWatch_III_Global_Wave_Model_best.ncd.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-N_0FWHrrv6",
        "cellView": "form"
      },
      "source": [
        "#@title All  classes and funcs.\n",
        "#---- Downloaders ----#\n",
        "class WW3downloaderascii:\n",
        "  #---- Parameters ----#\n",
        "  vars=['Thgt','Tper','Tdir']\n",
        "  pre_url='https://pae-paha.pacioos.hawaii.edu/thredds/dodsC/ww3_global/'\n",
        "  def __init__(self,iDate=None,eDate=None,lat=None,lon=None,\n",
        "               diriout=None,nameout=None,**kwargs):\n",
        "    self.iDate = iDate; self.eDate=eDate\n",
        "    nidays=datetime.strptime(iDate,'%Y-%m-%d')-datetime(2010,11,7)\n",
        "    nedays=datetime.strptime(eDate,'%Y-%m-%d')+timedelta(1)-datetime(2010,11,7)\n",
        "    ih=nidays.days*24+21; eh=nedays.days*24+21+23\n",
        "    ilat=np.arange(-77.5,77.5,.5).tolist().index(lat)\n",
        "    ilon=np.arange(0,359.5,.5).tolist().index(lon+180)\n",
        "    url=[f'WaveWatch_III_Global_Wave_Model_best.ncd.ascii?lon%5B{ilon}%5D,',\n",
        "         f'lat%5B{ilat}%5D,z%5B0:1:0%5D,time%5B{ih}:1:{eh}%5D,',\n",
        "         f'Thgt%5B{ih}:1:{eh}%5D%5B0:1:0%5D%5B{ilat}%5D%5B{ilon}%5D,',\n",
        "         f'Tper%5B{ih}:1:{eh}%5D%5B0:1:0%5D%5B{ilat}%5D%5B{ilon}%5D,',\n",
        "         f'Tdir%5B{ih}:1:{eh}%5D%5B0:1:0%5D%5B{ilat}%5D%5B{ilon}%5D']\n",
        "    self.lat=lat; self.lon=lon\n",
        "    self.url=''.join(map(str, url))\n",
        "    self.main_url=self.pre_url+self.url\n",
        "    self.diriout=diriout\n",
        "    self.nameout=nameout\n",
        "    self.copyright = 'Carlos Enciso Ojeda'\n",
        "    self.license = 'MIT@License'\n",
        "  @property\n",
        "  def downloadww3(self):\n",
        "    print(self.main_url)\n",
        "    try:\n",
        "      os.system(f'wget -c -P {self.diriout} {self.main_url}')\n",
        "    except:\n",
        "      print('Something went wrong. Check it!')\n",
        "      #---- Rename it ----#\n",
        "    fili=[os.path.join(self.diriout,x) for x in os.listdir(self.diriout) \\\n",
        "          if self.url[:4] in x]\n",
        "    os.rename(fili[0],self.diriout+self.nameout)\n",
        "  def ascii2csv(self):\n",
        "    #---- Converto CSV ----#\n",
        "    with open(self.diriout+self.nameout,'r+') as f:\n",
        "      file_lines=f.readlines()\n",
        "      Lines=list(file_lines)\n",
        "      #---- Looking for position ----#\n",
        "      pos_vars = {k:[n for n,x in enumerate(Lines) if f'{k}.' in x][:2] for k in self.vars}\n",
        "      contents = {k:Lines[v[0]:v[1]] for k,v in pos_vars.items()}\n",
        "      contents = {k:list(map(float,list(filter(None,[x.strip('\\n').split(',')[-1].strip() \\\n",
        "                                                     for x in v if k not in x])))) \\\n",
        "                                                     for k,v in contents.items()}\n",
        "      self.df = pd.DataFrame.from_dict(contents)\n",
        "      self.df['date'] = pd.date_range(self.iDate,periods=len(self.df),freq='1H')\n",
        "      self.df.set_index('date',inplace=True)\n",
        "      self.df=self.df.rename(columns={'Tdir':'dirpw','Tper':'perpw','Thgt':'swh'})\n",
        "#----------------------\n",
        "# Funcs using my class\n",
        "#----------------------\n",
        "def downloadASCII(iDate=None,eDate=None,diri_ascii=None,ascii_name=None,\n",
        "                  downloads=None,lat=None,lon=None):\n",
        "  os.makedirs(diri_ascii, exist_ok=True)\n",
        "  os.makedirs('./OUTPUT/', exist_ok=True)\n",
        "  nmnths = np.diff(np.array([datetime.strptime(x,'%Y-%m-%d') \\\n",
        "                             for x in [iDate,eDate]]))[0]\n",
        "  nmnths = nmnths.days/30\n",
        "  nameit = f'./OUTPUT/Dataset_lat_{ilat}_lon_{ilon}_{iDate}_to_{eDate}.csv'\n",
        "  #---- For downloading  ----#\n",
        "  if nmnths<=2:\n",
        "    ww3 = WW3downloaderascii(iDate=iDate,eDate=eDate,lat=-14,\n",
        "                             lon=-77,diriout=diri_ascii,nameout=ascii_name)\n",
        "    if downloads==True:\n",
        "      ww3.downloadww3\n",
        "    ww3.ascii2csv()\n",
        "    dfs=ww3.df\n",
        "  else:\n",
        "    idrg = pd.date_range(iDate,eDate,freq='1MS')\n",
        "    edrg = pd.date_range(iDate,eDate,freq='1M')\n",
        "    #---- Looping it 'cause it takes too much time if you want ----#\n",
        "    #---- to retrieve data for more than one year ----#\n",
        "    container=[]\n",
        "    for i,e in zip(idrg,edrg):\n",
        "      print(i.strftime('%Y-%m'))\n",
        "      #---- Download ascii ----#\n",
        "      ww3 = WW3downloaderascii(iDate=i.strftime('%Y-%m-%d'),\n",
        "                               eDate=e.strftime('%Y-%m-%d'),\n",
        "                               lat=ilat,lon=ilon,diriout=diri_ascii,\n",
        "                               nameout=ascii_name)\n",
        "      #---- May it takes its time depending the range time ----#\n",
        "      if downloads==True:\n",
        "        ww3.downloadww3\n",
        "      ww3.ascii2csv()\n",
        "      container.append(ww3.df)\n",
        "    dfs=pd.concat(container)\n",
        "  try:\n",
        "      dfs.to_csv(nameit)\n",
        "      print(f'Saveing... {nameit}')\n",
        "  except:\n",
        "      pass\n",
        "#---- Grib Funcs ----#\n",
        "def DownloadWW3(iDate,eDate,dirigrib=None):\n",
        "  \"\"\"\n",
        "    Introduce initial and end dates to download dataset\n",
        "    iDate: Initial Date in string format (YYYY-MM)\n",
        "    eDate: End Date in string format(YYYY-MM)\n",
        "  \"\"\"\n",
        "  os.makedirs(dirigrib, exist_ok=True)\n",
        "  #---- Range time ----#\n",
        "  drg = pd.date_range(iDate,eDate,freq='1MS')\n",
        "  #---- Setting parameters ----#\n",
        "  main_url = ['https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/',\n",
        "              'ftp://polar.ncep.noaa.gov/pub/history/waves/multi_1/']\n",
        "  vars = ['dp','hs','tp']\n",
        "  cname = ['multi_reanal.glo_30m_ext','multi_1.glo_30m']\n",
        "  #---- Split daterange ----#\n",
        "  for dts in drg:\n",
        "    if dts < datetime(2010,1,1):\n",
        "      urls=['{}{}/gribs/{}.{}.{}.grb2'.format(main_url[0],dts.strftime('%Y%m'),\n",
        "                                            cname[0],v,dts.strftime('%Y%m')) for v in vars]\n",
        "    elif dts >= datetime(2010,1,1) and dts <= datetime(2019,6,1):\n",
        "      urls=['{}{}/gribs/{}.{}.{}.grb2'.format(main_url[1],dts.strftime('%Y%m'),\n",
        "                                            cname[1],v,dts.strftime('%Y%m')) for v in vars]\n",
        "    else:\n",
        "      pass\n",
        "    #---- Download----#\n",
        "    try:\n",
        "      print('Downloading...')\n",
        "      print(urls)\n",
        "      _ = [os.system(f'wget -c -P {dirigrib} {x}') for x in urls]\n",
        "    except:\n",
        "      pass\n",
        "class postWW3grb2:\n",
        "  vars=['dp','hs','tp']\n",
        "  def __init__(self,lat=None,lon=None, poshindsubsets=False,\n",
        "               dirigrib=None,path_subset=None,):\n",
        "    self.dirigrib=dirigrib\n",
        "    self.path_subset=path_subset\n",
        "    os.makedirs(self.path_subset, exist_ok=True)\n",
        "    self.lat=lat; self.lon=lon+180 \n",
        "    self.poshindsubsets=poshindsubsets\n",
        "  #---- Preprocesing vars & time----#\n",
        "  def replacedtime(self,ds):\n",
        "    ds = ds.drop(['time'])\n",
        "    ds = ds.compute()\n",
        "    ds['step'] = ds['valid_time']\n",
        "    ds = ds.sel(step=~ds.get_index(\"step\").duplicated())\n",
        "    ds = ds.drop(['valid_time','surface'])\n",
        "    return ds\n",
        "  def poshindcastgrb2(self,htype='multi_',nameitnc='concated_',nameitcsv=None,nchunks=1000):\n",
        "    self.nameitnc=f'{nameitnc}_lat_{self.lat}_lon_{self.lon}.nc'\n",
        "    self.nameitcsv=f'./OUTPUT/{nameitcsv}'\n",
        "    if self.poshindsubsets:\n",
        "      self.dicfili={k:sorted([os.path.join(self.dirigrib,x) for x in os.listdir(self.dirigrib) \\\n",
        "                              if k in x and htype in x and x.endswith('.grb2')]) for k in self.vars}\n",
        "      dsdict={k:xr.open_mfdataset(v,combine='by_coords',concat_dim='time',preprocess=prepross,parallel=True,\n",
        "                                  chunks={'step':10000},engine='cfgrib') for k,v in self.dicfili.items()}\n",
        "      dsmf=xr.merge([v for v in dsdict.values()])\n",
        "      dsmf_rechunk = dsmf.chunk({'time':nchunks})\n",
        "      #---- Writing chunks ----#\n",
        "      print(f'Saving File: {self.path_subset+self.nameitnc}' )\n",
        "      delayed_obj = dsmf_rechunk.to_netcdf(self.path_subset+self.nameitnc, compute=False)\n",
        "      with ProgressBar():\n",
        "        results = delayed_obj.compute()\n",
        "    fili_subsets = [os.path.join(self.path_subset,x) for x in os.listdir(self.path_subset) if x.endswith('.nc') and self.nameitnc in x]\n",
        "    ds = xr.open_mfdataset(fili_subsets)\n",
        "    ds = ds.rename({'time':'date'})\n",
        "    df = ds.to_dataframe()\n",
        "    df = df.drop(columns=['latitude','longitude'])\n",
        "    df = df[~df.index.duplicated()]\n",
        "    df.sort_index(inplace=True)\n",
        "    df.to_csv(self.nameitcsv)\n",
        "#---- Merging all Dataset ----#\n",
        "def merging_all(outdiri='./OUTPUT/',nameit=None):\n",
        "  fili_csv = [os.path.join(outdiri,x) for x in os.listdir(outdiri) if x.endswith('.csv') and 'Dataset' in x]\n",
        "  dfcat = pd.concat([pd.read_csv(x) for x in fili_csv])\n",
        "  dfcat.sort_values('date',inplace=True)\n",
        "  dfcat['date']=pd.to_datetime(dfcat['date'],infer_datetime_format=True)\n",
        "  dfcat.set_index('date',inplace=True)\n",
        "  dfcat = dfcat.resample('3H').mean()\n",
        "  dfcat = dfcat.round(3)\n",
        "  dfcat.to_csv(outdiri+nameit)\n",
        "  return dfcat\n",
        "def myheader(outdiri='./OUTPUT/',nameit=None):\n",
        "  Lines = ['#'+'-'*80,\n",
        "           '# -*- coding:utf-8 -*-',\n",
        "           '# %Project       : Pisco Project WW3',\n",
        "           '# %Author       : Carlos Enciso Ojeda',\n",
        "           '# %Email        : carlos.enciso.o@gmail.com',\n",
        "           '# %Created Date : Tuesday, April 03th 2021, 12:22:40 am',\n",
        "           '# %Copyright (c): 2021 EyM GeoInsight',\n",
        "           '# %License      : MIT',\n",
        "           '#------------#','# Parameters:','#------------#',\n",
        "           '# .swh.   = significant height of combined wind waves and swell (m)',\n",
        "           '# .perpw. = primary wave mean period (s)',\n",
        "           '# .dirpw. = primary wave direction',\n",
        "           '# (degrees true, i.e. 0 deg => coming from North; 90 deg => coming from East)',\n",
        "           '#--------------------------------------------------------------------------------']\n",
        "  nlns=len(Lines)\n",
        "  with open(outdiri+nameit,'r+') as f:\n",
        "    lines=f.readlines()\n",
        "    nls=len(lines)\n",
        "    f.seek(0)\n",
        "    for n,l in enumerate(Lines):\n",
        "      f.write(l+'\\n')\n",
        "    for line in lines:\n",
        "      f.write(line)\n",
        "    f.close()\n",
        "# ---- Pass it ----#\n",
        "if __name__ == '__main__':\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9U4xSoBNE8Z"
      },
      "source": [
        "### **2.1 Download Gribs**\n",
        "Available Dataset from 1979-01 to 2010-12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adDi-ZfY3WzY"
      },
      "source": [
        "#---- Cleaning directories ----#\n",
        "!rm -rf ./DATASETS/\n",
        "!rm -rf ./OUTPUT/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bw5UEQiNQhu"
      },
      "source": [
        "dirigrib='./DATASETS/GRIB2/'\n",
        "DownloadWW3('1979-01','1979-03',dirigrib=dirigrib)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siJeGqghOhU2"
      },
      "source": [
        "### **2.2 Download ASCII**\n",
        "Available Dataset from 2011-01 to 2021-02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLfKXj_nNQfZ"
      },
      "source": [
        "diri_ascii='./DATASETS/ASCII/'\n",
        "ascii_name='WW3ascii'\n",
        "ilat=-14\n",
        "ilon=-77\n",
        "iDate='2020-11-01'\n",
        "eDate='2021-02-01'\n",
        "downloadASCII(iDate=iDate,eDate=eDate,diri_ascii=diri_ascii,ascii_name=ascii_name,\n",
        "              downloads=True,lat=ilat,lon=ilon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCTM8yYEUv96"
      },
      "source": [
        "### **2.3 PostProcessing Gribs**\n",
        "Postprocess All Available Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZDvlQkvNQdN"
      },
      "source": [
        "#---- Using my class ----#\n",
        "dirigrib='./DATASETS/GRIB2/'\n",
        "path_subset='./DATASETS/netCDF/'\n",
        "ilat=-14\n",
        "ilon=-77\n",
        "#--------------- Dont touch this ---------------#\n",
        "def prepross(ds,lat=ilat,lon=ilon):\n",
        "    ds['step']=ds['time']+ds['step']\n",
        "    ds=ds.drop(['time','valid_time','surface'])\n",
        "    ds=ds.rename({'step':'time'})\n",
        "    ds=ds.sel(latitude=lat,longitude=lon+180)\n",
        "    return ds\n",
        "#-----------------------------------------------# \n",
        "nameit=f'Dataset_postGrib2_lat_{ilat}_lon_{ilon}.csv'\n",
        "postww3 = postWW3grb2(lat=ilat,lon=ilon,poshindsubsets=True,\n",
        "                      dirigrib=dirigrib,path_subset=path_subset)\n",
        "postww3.poshindcastgrb2(nameitcsv=nameit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLXR1qmtiL8u"
      },
      "source": [
        "### **2.4 Merging all CSV**\n",
        "Merging all dataset within the OUTPUT directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G_mWeJANQbC"
      },
      "source": [
        "#---- Merging all CSV ----#\n",
        "outdiri='./OUTPUT/'\n",
        "nameit='WW3_Allmerged_dataset.csv'\n",
        "merging_all(outdiri=outdiri,nameit=nameit)\n",
        "#---- Writting my header ----#\n",
        "fili=outdiri+nameit\n",
        "myheader(outdiri=outdiri,nameit=nameit)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}